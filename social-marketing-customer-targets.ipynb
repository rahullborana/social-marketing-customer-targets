{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Market segmentation","metadata":{}},{"cell_type":"markdown","source":"This information was gathered as part of a market research study using followers of a well-known consumer brand's Twitter account, which we'll call \"NutrientH20\" for the sake of labeling it. In order to sharpen its messaging a little bit further, NutrientH20 wanted to gain a little bit more insight into its social media audience.\n\nA little context on the data collection: a sample of the brand's Twitter followers were collected by the advertising company that manages NutrientH20's online marketing campaigns. Over the course of a week in June 2014, they gathered every tweet (\"tweet\") made by each of those followers. A human annotator hired by Amazon's Mechanical Turk service reviewed each post. \n\nEach of the 36 pre-defined categories represented a broad area of interest (e.g., politics, sports, family, etc.), and were used to categorize each tweet based on its content. A post could have multiple categories assigned to it by annotators. For illustration, a fictitious post like \"I'm really excited to see grandpa go wreck shop in his geriatic soccer league this Sunday!\" might be classified as both \"family\" and \"sports.\" You see what I mean.\n\nA random (anonymous, unique) 9-digit alphanumeric code assigned to each entry in social_marketing.csv designates a single user. The interests represented by each column are identified by labels at the top of the data file. The number of posts made by a specific user that fit the specified category is represented by the entries.\n\nNot all posts will have accurate annotations. Some annotators may have merely fallen asleep at the wheel occasionally or perhaps often! As a result, some error and noise are unavoidable during the annotation process.\n\nLet's assess this data and write a brief report for NutrientH20 that identifies any intriguing market segments that stand out in their social media audience. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf= pd.read_csv('social_marketing.csv')\ndf.rename(columns={'Unnamed: 0':'user'}, inplace=True)\ndf.head()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping the columns of chatter and uncategorized as they do not explain any useful user characteristics\ndf.drop(columns=['uncategorized'], axis=1, inplace=True)\ndf.drop(columns=['chatter'], axis=1, inplace=True)\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_df = df.drop(columns=['user'])\n\n# Get the column name with the maximum value for each row\ndf['topic'] = numeric_df.idxmax(axis=1)\n\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking if there is any user who has 0 interests along all the column\n\nfeatures_df= df.drop(columns=['user','topic'],axis=1)\nfeatures_df.loc[(df==0).all(axis=1)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n\n# features_df = StandardScaler().fit_transform(features_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca= PCA(n_components=25, random_state=41)\npcs= pca.fit_transform(features_df)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the explained variances\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_ratio_, color='black')\nplt.xlabel('PCA features')\nplt.ylabel('variance %')\nplt.xticks(features)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looks like PC1 to PC5 explain most of the variablility, lets calculate how much\nprint(pca.explained_variance_ratio_[:6].sum())\n# It's ~70%, lets add some more so that we can explain around 85%\n\nprint(pca.explained_variance_ratio_[:13].sum())\n\n# Let's choose PC1 through PC12\n\n# Save components to a DataFrame\nPCA_components = pd.DataFrame(pcs)\nPCA_components=PCA_components.iloc[:, : 13]\n\nPCA_components.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trying to find groups through scatter plots if evident clusters are found between principal components 1 to 5\n\n# Create scatter plots between 5 columns\nsns.set(style=\"ticks\")\nsns.pairplot(PCA_components[[0,1,2,3,4]], diag_kind='kde',corner=True)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Could not observe anything significant, lets try to visualize the Principal components through tSNE","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, verbose=1, perplexity=30, n_iter=1000, learning_rate=200)\ntsne_results = tsne.fit_transform(PCA_components)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.scatter(tsne_results[:, 0], tsne_results[:, 1])\nplt.xticks([])\nplt.yticks([])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's create an interactive visualization using plotly based on the customer topics which we assumed earlier\n\nimport plotly.express as px\nimport random\n# convert the t-SNE results to a DataFrame\ntsne_df = pd.DataFrame(data = tsne_results, columns = ['Dim1', 'Dim2'])\n\n# add the county names to this DataFrame\ntsne_df['Topic'] = df['topic'].values\n\ntopic_counts = df['topic'].value_counts()\n\n\n# Get the top 75% topics by count\ntop_topics = topic_counts.index[:round(len(df['topic'].unique())*0.75)]\n\n# Create a dictionary to map topic to color\ncolor_map = {topic: 'white' if topic not in top_topics else \"#{:06x}\".format(random.randint(0, 0xFFFFFF)) for topic in tsne_df['Topic'].unique() if topic != 'white'}\n\n# Create an interactive plot and color the points based on the color map\nfig = px.scatter(tsne_df, x='Dim1', y='Dim2', hover_data=['Topic'], color='Topic', color_discrete_map=color_map)\n\n# Set the height of the plot to elongate it\nfig.update_layout(height=800)\n\n# Show the plot\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alhough we can get the most frequent topics just by value counts, these topics can be far apart in the Dim1-Dim2 plot\n\nJust by the look of the plot, we can make the following observations about the user clusters\n\nBy looking at this plot, in the order of size, we can observe big clusters of \n - Photo Sharing with Shopping and Current events\n - Health Nutrition with personal fitness\n - Cooking\n - College uni and online gaming\n - News, politics and travel","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_scores = []\ninertia_values = []\n\nfor k in range(2, 11):  # Start from 2 clusters as silhouette score requires at least 2 clusters\n    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n    kmeans.fit(PCA_components)\n    \n    # Calculate silhouette score and inertia for each k\n    silhouette_scores.append(silhouette_score(PCA_components, kmeans.labels_))\n    inertia_values.append(kmeans.inertia_)\n\n# Plot silhouette scores\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(range(2, 11), silhouette_scores, marker='o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score for K-means Clustering')\n\n# Plot the elbow curve\nplt.subplot(1, 2, 2)\nplt.plot(range(2, 11), inertia_values, marker='o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Curve for K-means Clustering')\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the above plots, lets try to visualize the clusters based on 4 and 5 clusters respectively","metadata":{}},{"cell_type":"code","source":"# Perform K-means clustering with the optimal number of clusters with initializatio\nkmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\ncluster_labels = kmeans.fit_predict(PCA_components)\n\nPCA_components_original= PCA_components.copy()\n\nPCA_components['cluster'] = cluster_labels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Merge the original dataframe with cluster information to analyze the clusters\nconcatenated_df = pd.concat([df, PCA_components[['cluster']]], axis=1)\n\n#We do not need our user column and our topic column that we previously created\nconcatenated_df.drop(columns=['user','topic'], axis=1, inplace=True)\n\nconcatenated_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate mean values for each cluster to analyze prominance of certain variables in each cluster\ncluster_analysis = concatenated_df.groupby('cluster').mean()  \ncluster_analysis.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's try to get the top 4 topics by per cluster based on the mean values along with the cluster sizes\n\ncluster_info_dict = {}\n\n# Iterate over each cluster\nfor cluster in cluster_analysis.index:\n    # Sort the topics within the cluster based on their mean values\n    sorted_topics = cluster_analysis.loc[cluster].sort_values(ascending=False)\n    \n    # Extract the top 3 topics (excluding 'cluster' column)\n    top_topics = sorted_topics.index[0:4].tolist()  # Corrected index range\n\n    # Extract corresponding mean values for those topics\n    top_values = sorted_topics.values[0:4].tolist()\n    \n    # Calculate the cluster size (number of data points in the cluster)\n    cluster_size = len(concatenated_df[concatenated_df['cluster'] == cluster])\n    \n    # Store the top topics and cluster size in the dictionary\n    cluster_info_dict[cluster] = {'top_topics': top_topics, 'cluster_size': cluster_size, 'top_values':top_values}\n\n# Display the top topics and cluster sizes for each cluster\nfor cluster, info in cluster_info_dict.items():\n    top_topics = ', '.join(info['top_topics'])\n    top_values = info['top_values']\n    cluster_size = info['cluster_size']\n    print(f\"Cluster {cluster}: Top 4 Topics - {top_topics}, Corresponding mean values- {top_values}, Cluster Size: {cluster_size}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting top topics per cluster \n\n\n# Create a grid of subplots\nnum_clusters = len(cluster_info_dict)\nrows = num_clusters // 2  \ncols = 2  \nfig, axes = plt.subplots(rows, cols, figsize=(15, 10))  \n\nif rows == 1:\n    axes = [axes]\n\n# Iterate over each cluster and its corresponding subplot\nfor i, (cluster, info) in enumerate(cluster_info_dict.items()):\n    row = i // cols\n    col = i % cols\n    \n    top_topics = info['top_topics']\n    top_values = info['top_values']\n    cluster_size = info['cluster_size']\n    \n    # Plot the top topics for the current cluster on the corresponding subplot\n    axes[row][col].barh(top_topics, top_values, color='skyblue')\n    axes[row][col].set_xlabel('Mean Values')\n    axes[row][col].set_ylabel('Topics')\n    axes[row][col].set_title(f'Cluster {cluster}')\n    \n    # Display the cluster size as a text annotation\n    axes[row][col].annotate(f'Cluster Size: {cluster_size}', xy=(0.5, 0.02),\n                            xycoords='axes fraction', ha='center', fontsize=10)\n    \n# Adjust layout for better spacing between subplots\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This cluster analysis revealed distinct topic preferences and strengths within different clusters.\n\nCluster 0: Centered on \"college_uni\" and \"online_gaming,\" with a focus on \"photo_sharing\" and \"sports_playing,\" reflecting an interest in education, gaming, and sports.\n\nCluster 1: Emphasizes \"photo_sharing\" and \"politics,\" while also showing interest in \"sports_fandom\" and \"travel,\" suggesting engagement in current affairs, sports, and travel.\n`\nCluster 2: Primarily revolves around \"health_nutrition\" and \"personal_fitness,\" accompanied by interest in \"cooking\" and \"photo_sharing,\" signifying health-conscious behaviors and culinary interests.\n\nCluster 3: Showcases a blend of \"cooking,\" \"photo_sharing,\" \"fashion,\" and \"beauty,\" revealing an affinity for cooking, fashion trends, and beauty-related content.","metadata":{}},{"cell_type":"code","source":"# Perform clustering with the optimal number of clusters with k-means++ initialization\nkmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)\n\nPCA_components = PCA_components_original.copy()\ncluster_labels = kmeans.fit_predict(PCA_components)\n\n\n\nPCA_components['cluster'] = cluster_labels\n\n#Merge the original dataframe with cluster information to analyze the clusters\nconcatenated_df = pd.concat([df, PCA_components[['cluster']]], axis=1)\n\n#We do not need our user column and our topic column that we previously created\nconcatenated_df.drop(columns=['user','topic'], axis=1, inplace=True)\n\ncluster_analysis = concatenated_df.groupby('cluster').mean()  # Calculate mean values for each cluster\n\ncluster_info_dict = {}\n\n# Iterate over each cluster\nfor cluster in cluster_analysis.index:\n    # Sort the topics within the cluster based on their mean values\n    sorted_topics = cluster_analysis.loc[cluster].sort_values(ascending=False)\n    \n    # Extract the top 3 topics (excluding 'cluster' column)\n    top_topics = sorted_topics.index[0:4].tolist()  # Corrected index range\n\n    # Extract corresponding mean values for those topics\n    top_values = sorted_topics.values[0:4].tolist()\n    \n    # Calculate the cluster size (number of data points in the cluster)\n    cluster_size = len(concatenated_df[concatenated_df['cluster'] == cluster])\n    \n    # Store the top topics and cluster size in the dictionary\n    cluster_info_dict[cluster] = {'top_topics': top_topics, 'cluster_size': cluster_size, 'top_values':top_values}\n\n# Display the top topics and cluster sizes for each cluster\nfor cluster, info in cluster_info_dict.items():\n    top_topics = ', '.join(info['top_topics'])\n    top_values = info['top_values']\n    cluster_size = info['cluster_size']\n    print(f\"Cluster {cluster}: Top 4 Topics - {top_topics}, Corresponding mean values- {top_values}, Cluster Size: {cluster_size}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting top topics per cluster \n\n\n# Create a grid of subplots\nnum_clusters = len(cluster_info_dict)\nrows = num_clusters // 2  \ncols = 2  \nfig, axes = plt.subplots(rows, cols, figsize=(15, 10))  \n\nif rows == 1:\n    axes = [axes]\n\n# Iterate over each cluster and its corresponding subplot\nfor i, (cluster, info) in enumerate(cluster_info_dict.items()):\n    row = i // cols\n    col = i % cols\n    \n    top_topics = info['top_topics']\n    top_values = info['top_values']\n    cluster_size = info['cluster_size']\n    \n    # Plot the top topics for the current cluster on the corresponding subplot\n    axes[row][col].barh(top_topics, top_values, color='skyblue')\n    axes[row][col].set_xlabel('Mean Values')\n    axes[row][col].set_ylabel('Topics')\n    axes[row][col].set_title(f'Cluster {cluster}')\n    \n    # Display the cluster size as a text annotation\n    axes[row][col].annotate(f'Cluster Size: {cluster_size}', xy=(0.5, 0.02),\n                            xycoords='axes fraction', ha='center', fontsize=10)\n    \n# Adjust layout for better spacing between subplots\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This cluster analysis provides insights into diverse topic preferences and strengths within distinct clusters.\n\nCluster 0: Focused on \"cooking,\" \"photo_sharing,\" \"fashion,\" and \"beauty,\" showcasing a combination of interests in culinary arts and style.\n\nCluster 1: Highlights \"health_nutrition\" and \"personal_fitness,\" coupled with \"cooking\" and \"photo_sharing,\" indicating a health-conscious segment with culinary inclinations.\n\nCluster 2: Centers on \"college_uni\" and \"online_gaming,\" accompanied by \"photo_sharing\" and \"sports_playing,\" representing an audience engaged in education, gaming, and sports.\n\nCluster 3: Emphasizes \"photo_sharing,\" \"sports_fandom,\" \"current_events,\" and \"shopping,\" suggesting an interest in photography, sports, current affairs, and shopping.\n\nCluster 4: Showcases \"politics,\" \"travel,\" \"news,\" and \"photo_sharing,\" portraying an engaged group interested in politics, travel, and current events.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import silhouette_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nPCA_components = PCA_components_original.copy()\n\nsilhouette_scores = []\ninertia_values = []\n\nfor k in range(2, 11):  # Start from 2 clusters as silhouette score requires at least 2 clusters\n    spectral_clustering = SpectralClustering(n_clusters=k, random_state=42, affinity='nearest_neighbors')\n    labels = spectral_clustering.fit_predict(PCA_components)\n    \n    # Calculate silhouette score\n    silhouette_scores.append(silhouette_score(PCA_components, labels))\n    \n    # Inertia is not applicable for spectral clustering, but you can use a placeholder value\n    inertia_values.append(0.0)\n\n# Plot silhouette scores\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(range(2, 11), silhouette_scores, marker='o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score for Spectral Clustering')\n\n# Plot the elbow curve (use placeholder values)\nplt.subplot(1, 2, 2)\nplt.plot(range(2, 11), inertia_values, marker='o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Curve for Spectral Clustering')\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this Silhoutte plot we will try the optimal number of clusters as 4 and 5","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import SpectralClustering\n\n\n# Specify the number of clusters for spectral clustering\nn_clusters = 4\n\n# Initialize SpectralClustering\nspectral_clustering = SpectralClustering(n_clusters=n_clusters, random_state=42, affinity='nearest_neighbors')\n\n# Perform spectral clustering and get cluster labels\ncluster_labels = spectral_clustering.fit_predict(PCA_components_original)\n\n# Copy PCA_components_original to avoid modifying the original data\nPCA_components = PCA_components_original.copy()\n\n# Add cluster labels to PCA_components DataFrame\nPCA_components['cluster'] = cluster_labels\n\n# Merge the original dataframe with cluster information to analyze the clusters\nconcatenated_df = pd.concat([df, PCA_components[['cluster']]], axis=1)\n\n# Drop unnecessary columns\nconcatenated_df.drop(columns=['user', 'topic'], axis=1, inplace=True)\n\n# Calculate mean values for each cluster\ncluster_analysis = concatenated_df.groupby('cluster').mean()\n\n# Create a dictionary to store cluster information\ncluster_info_dict = {}\n\n# Iterate over each cluster\nfor cluster in cluster_analysis.index:\n    # Sort the topics within the cluster based on their mean values\n    sorted_topics = cluster_analysis.loc[cluster].sort_values(ascending=False)\n    \n    # Extract the top 4 topics (excluding 'cluster' column)\n    top_topics = sorted_topics.index[0:4].tolist()\n\n    # Extract corresponding mean values for those topics\n    top_values = sorted_topics.values[0:4].tolist()\n    \n    # Calculate the cluster size (number of data points in the cluster)\n    cluster_size = len(concatenated_df[concatenated_df['cluster'] == cluster])\n    \n    # Store the top topics and cluster size in the dictionary\n    cluster_info_dict[cluster] = {'top_topics': top_topics, 'cluster_size': cluster_size, 'top_values': top_values}\n\n# Display the top topics and cluster sizes for each cluster\nfor cluster, info in cluster_info_dict.items():\n    top_topics = ', '.join(info['top_topics'])\n    top_values = info['top_values']\n    cluster_size = info['cluster_size']\n    print(f\"Cluster {cluster}: Top 4 Topics - {top_topics}, Corresponding mean values - {top_values}, Cluster Size: {cluster_size}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting top topics per cluster \n\n\n# Create a grid of subplots\nnum_clusters = len(cluster_info_dict)\nrows = num_clusters // 2  \ncols = 2  \nfig, axes = plt.subplots(rows, cols, figsize=(15, 10))  \n\nif rows == 1:\n    axes = [axes]\n\n# Iterate over each cluster and its corresponding subplot\nfor i, (cluster, info) in enumerate(cluster_info_dict.items()):\n    row = i // cols\n    col = i % cols\n    \n    top_topics = info['top_topics']\n    top_values = info['top_values']\n    cluster_size = info['cluster_size']\n    \n    # Plot the top topics for the current cluster on the corresponding subplot\n    axes[row][col].barh(top_topics, top_values, color='skyblue')\n    axes[row][col].set_xlabel('Mean Values')\n    axes[row][col].set_ylabel('Topics')\n    axes[row][col].set_title(f'Cluster {cluster}')\n    \n    # Display the cluster size as a text annotation\n    axes[row][col].annotate(f'Cluster Size: {cluster_size}', xy=(0.5, 0.02),\n                            xycoords='axes fraction', ha='center', fontsize=10)\n    \n# Adjust layout for better spacing between subplots\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This cluster analysis uncovers significant topic preferences within distinct clusters.\n\nCluster 0: Focused on \"college_uni,\" \"online_gaming,\" \"photo_sharing,\" and \"sports_playing,\" indicating an audience engaged in education, gaming, and sports-related activities.\n\nCluster 1: Highlights \"health_nutrition,\" \"personal_fitness,\" \"cooking,\" and \"photo_sharing,\" suggesting a health-conscious segment with culinary interests.\n\nCluster 2: Centers around \"photo_sharing,\" \"sports_fandom,\" \"cooking,\" and \"current_events,\" depicting an audience enthusiastic about photography, sports, and current affairs.\n\nCluster 3: Emphasizes \"politics,\" \"travel,\" \"news,\" and \"computers,\" revealing a tech-savvy group interested in politics, travel, and technology.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import SpectralClustering\n\n\n# Specify the number of clusters for spectral clustering\nn_clusters = 5\n\n# Initialize SpectralClustering\nspectral_clustering = SpectralClustering(n_clusters=n_clusters, random_state=42, affinity='nearest_neighbors')\n\n# Perform spectral clustering and get cluster labels\ncluster_labels = spectral_clustering.fit_predict(PCA_components_original)\n\n# Copy PCA_components_original to avoid modifying the original data\nPCA_components = PCA_components_original.copy()\n\n# Add cluster labels to PCA_components DataFrame\nPCA_components['cluster'] = cluster_labels\n\n# Merge the original dataframe with cluster information to analyze the clusters\nconcatenated_df = pd.concat([df, PCA_components[['cluster']]], axis=1)\n\n# Drop unnecessary columns\nconcatenated_df.drop(columns=['user', 'topic'], axis=1, inplace=True)\n\n# Calculate mean values for each cluster\ncluster_analysis = concatenated_df.groupby('cluster').mean()\n\n# Create a dictionary to store cluster information\ncluster_info_dict = {}\n\n# Iterate over each cluster\nfor cluster in cluster_analysis.index:\n    # Sort the topics within the cluster based on their mean values\n    sorted_topics = cluster_analysis.loc[cluster].sort_values(ascending=False)\n    \n    # Extract the top 4 topics (excluding 'cluster' column)\n    top_topics = sorted_topics.index[0:4].tolist()\n\n    # Extract corresponding mean values for those topics\n    top_values = sorted_topics.values[0:4].tolist()\n    \n    # Calculate the cluster size (number of data points in the cluster)\n    cluster_size = len(concatenated_df[concatenated_df['cluster'] == cluster])\n    \n    # Store the top topics and cluster size in the dictionary\n    cluster_info_dict[cluster] = {'top_topics': top_topics, 'cluster_size': cluster_size, 'top_values': top_values}\n\n# Display the top topics and cluster sizes for each cluster\nfor cluster, info in cluster_info_dict.items():\n    top_topics = ', '.join(info['top_topics'])\n    top_values = info['top_values']\n    cluster_size = info['cluster_size']\n    print(f\"Cluster {cluster}: Top 4 Topics - {top_topics}, Corresponding mean values - {top_values}, Cluster Size: {cluster_size}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting top topics per cluster \n\n# Create a grid of subplots\nnum_clusters = len(cluster_info_dict)\nrows = num_clusters // 2  \ncols = 2  \nfig, axes = plt.subplots(rows, cols, figsize=(15, 10))  \n\nif rows == 1:\n    axes = [axes]\n\n# Iterate over each cluster and its corresponding subplot\nfor i, (cluster, info) in enumerate(cluster_info_dict.items()):\n    row = i // cols\n    col = i % cols\n    \n    top_topics = info['top_topics']\n    top_values = info['top_values']\n    cluster_size = info['cluster_size']\n    \n    # Plot the top topics for the current cluster on the corresponding subplot\n    axes[row][col].barh(top_topics, top_values, color='skyblue')\n    axes[row][col].set_xlabel('Mean Values')\n    axes[row][col].set_ylabel('Topics')\n    axes[row][col].set_title(f'Cluster {cluster}')\n    \n    # Display the cluster size as a text annotation\n    axes[row][col].annotate(f'Cluster Size: {cluster_size}', xy=(0.5, 0.02),\n                            xycoords='axes fraction', ha='center', fontsize=10)\n    \n# Adjust layout for better spacing between subplots\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cluster 0: Centered on \"health_nutrition,\" \"personal_fitness,\" \"cooking,\" and \"photo_sharing,\" this group exhibits a health-conscious focus with culinary and wellness interests.\n\nCluster 1: Emphasizes \"photo_sharing,\" \"sports_fandom,\" \"current_events,\" and \"shopping,\" indicating an engagement with photography, sports, current affairs, and shopping.\n\nCluster 2: Highlights \"college_uni,\" \"online_gaming,\" \"photo_sharing,\" and \"sports_playing,\" suggesting an audience interested in education, online gaming, and sports-related content.\n\nCluster 3: Focused on \"politics,\" \"travel,\" \"news,\" and \"computers,\" this cluster indicates an engagement with political news, travel content, and technology.\n\nCluster 4: Revolves around \"cooking,\" \"photo_sharing,\" \"fashion,\" and \"beauty,\" portraying interests in culinary arts, photography, fashion, and beauty.","metadata":{}},{"cell_type":"markdown","source":"**Conclusion**\n\nIn this analysis, we employed spectral clustering to segment a dataset based on user preferences across different topics. The dataset was first transformed using Principal Component Analysis (PCA) for dimension reduction. Subsequently, K-means and SpectralClustering was employed with varying cluster counts to identify distinctive user clusters. The resulting clusters reveal meaningful insights that can guide business strategies for NutrientH20.\n\nAcross all our analyses, clusters exhibit distinct interests and corresponding mean values, enabling targeted business strategies:\n\nFitness Focussed: Centers around health, fitness, cooking, and photo sharing. This presents opportunities for health and wellness brands, recipe platforms, and fitness apps to engage this health-conscious group.\n\nSocial Media Focused: Shows affinity for social sharing, politics, and travel. Tailored content creation and partnerships with travel and news platforms can effectively resonate with this socially and politically engaged segment.\n\nEducational and Sports focused: Displays interests in education, gaming, and sports. Businesses in the education sector and sports industry can capitalize on this cluster by offering relevant content, courses, and gaming experiences.\n\nLifestyle Focused: Prioritizes lifestyle topics like fashion, beauty, and shopping. E-commerce platforms, beauty brands, and fashion retailers can optimize marketing to captivate this style-conscious audience.\n\n\nBusiness Implications:\nThe clustering analysis provides valuable insights for tailored marketing strategies, content creation, and product offerings for NutrientH20. They can personalize user experiences, targeting each cluster with relevant content and promotions. For instance, health and wellness brands can collaborate with influencers from 'Fitness Focused' cluster to amplify their message. E-commerce platforms can curate product recommendations, and News agencies can tailor news updates for 'Social Media Focused' cluster , while educational institutions can engage 'Educational and Sports focused' with suitable offerings. Fashion brands can collaborate with 'Lifestyle Focused' enthusiasts. By understanding and leveraging these user clusters, NutrientH20 can enhance customer engagement and drive growth by delivering precisely what their users are interested in.","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}